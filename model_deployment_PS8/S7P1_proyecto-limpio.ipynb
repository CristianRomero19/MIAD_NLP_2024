{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2 - Clasificación de género de películas\n",
    "\n",
    "El propósito de este proyecto es que puedan poner en práctica, en sus respectivos grupos de trabajo, sus conocimientos sobre técnicas de preprocesamiento, modelos predictivos de NLP, y la disponibilización de modelos. Para su desarrollo tengan en cuenta las instrucciones dadas en la \"Guía del proyecto 2: Clasificación de género de películas\"\n",
    "\n",
    "**Entrega**: La entrega del proyecto deberán realizarla durante la semana 8. Sin embargo, es importante que avancen en la semana 7 en el modelado del problema y en parte del informe, tal y como se les indicó en la guía.\n",
    "\n",
    "Para hacer la entrega, deberán adjuntar el informe autocontenido en PDF a la actividad de entrega del proyecto que encontrarán en la semana 8, y subir el archivo de predicciones a la [competencia de Kaggle](https://www.kaggle.com/t/2c54d005f76747fe83f77fbf8b3ec232)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos para la predicción de género en películas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/moviegenre.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto se usará un conjunto de datos de géneros de películas. Cada observación contiene el título de una película, su año de lanzamiento, la sinopsis o plot de la película (resumen de la trama) y los géneros a los que pertenece (una película puede pertenercer a más de un género). Por ejemplo:\n",
    "- Título: 'How to Be a Serial Killer'\n",
    "- Plot: 'A serial killer decides to teach the secrets of his satisfying career to a video store clerk.'\n",
    "- Generos: 'Comedy', 'Crime', 'Horror'\n",
    "\n",
    "La idea es que usen estos datos para predecir la probabilidad de que una película pertenezca, dada la sinopsis, a cada uno de los géneros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agradecemos al profesor Fabio González, Ph.D. y a su alumno John Arevalo por proporcionar este conjunto de datos. Ver https://arxiv.org/abs/1702.01992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo predicción conjunto de test para envío a Kaggle\n",
    "En esta sección encontrarán el formato en el que deben guardar los resultados de la predicción para que puedan subirlos a la competencia en Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, roc_auc_score, make_scorer, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import metrics\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from livelossplot import PlotLossesKeras\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "\n",
    "import joblib\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ft_model = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>genres</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>2003</td>\n",
       "      <td>Most</td>\n",
       "      <td>most is the story of a single father who takes...</td>\n",
       "      <td>['Short', 'Drama']</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>2008</td>\n",
       "      <td>How to Be a Serial Killer</td>\n",
       "      <td>a serial killer decides to teach the secrets o...</td>\n",
       "      <td>['Comedy', 'Crime', 'Horror']</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>1941</td>\n",
       "      <td>A Woman's Face</td>\n",
       "      <td>in sweden ,  a female blackmailer with a disfi...</td>\n",
       "      <td>['Drama', 'Film-Noir', 'Thriller']</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>1954</td>\n",
       "      <td>Executive Suite</td>\n",
       "      <td>in a friday afternoon in new york ,  the presi...</td>\n",
       "      <td>['Drama']</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>1990</td>\n",
       "      <td>Narrow Margin</td>\n",
       "      <td>in los angeles ,  the editor of a publishing h...</td>\n",
       "      <td>['Action', 'Crime', 'Thriller']</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                      title  \\\n",
       "3107  2003                       Most   \n",
       "900   2008  How to Be a Serial Killer   \n",
       "6724  1941             A Woman's Face   \n",
       "4704  1954            Executive Suite   \n",
       "2582  1990              Narrow Margin   \n",
       "\n",
       "                                                   plot  \\\n",
       "3107  most is the story of a single father who takes...   \n",
       "900   a serial killer decides to teach the secrets o...   \n",
       "6724  in sweden ,  a female blackmailer with a disfi...   \n",
       "4704  in a friday afternoon in new york ,  the presi...   \n",
       "2582  in los angeles ,  the editor of a publishing h...   \n",
       "\n",
       "                                  genres  rating  \n",
       "3107                  ['Short', 'Drama']     8.0  \n",
       "900        ['Comedy', 'Crime', 'Horror']     5.6  \n",
       "6724  ['Drama', 'Film-Noir', 'Thriller']     7.2  \n",
       "4704                           ['Drama']     7.4  \n",
       "2582     ['Action', 'Crime', 'Thriller']     6.6  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualización datos de entrenamiento\n",
    "dataTraining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>Message in a Bottle</td>\n",
       "      <td>who meets by fate ,  shall be sealed by fate ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1978</td>\n",
       "      <td>Midnight Express</td>\n",
       "      <td>the true story of billy hayes ,  an american c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1996</td>\n",
       "      <td>Primal Fear</td>\n",
       "      <td>martin vail left the chicago da ' s office to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1950</td>\n",
       "      <td>Crisis</td>\n",
       "      <td>husband and wife americans dr .  eugene and mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1959</td>\n",
       "      <td>The Tingler</td>\n",
       "      <td>the coroner and scientist dr .  warren chapin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                title  \\\n",
       "1  1999  Message in a Bottle   \n",
       "4  1978     Midnight Express   \n",
       "5  1996          Primal Fear   \n",
       "6  1950               Crisis   \n",
       "7  1959          The Tingler   \n",
       "\n",
       "                                                plot  \n",
       "1  who meets by fate ,  shall be sealed by fate ....  \n",
       "4  the true story of billy hayes ,  an american c...  \n",
       "5  martin vail left the chicago da ' s office to ...  \n",
       "6  husband and wife americans dr .  eugene and mr...  \n",
       "7  the coroner and scientist dr .  warren chapin ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualización datos de test\n",
    "dataTesting.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una función de preprocesameinto de texto, la cual convierte todo el texto en minúscula y elimina signos de puntuación que puedan intervenir en la tokenización de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se define la función\n",
    "\n",
    "def preprocesamiento_texto(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar preprocesamiento\n",
    "\n",
    "dataTraining['plot'] = dataTraining['plot'].apply(preprocesamiento_texto)\n",
    "dataTraining['title'] = dataTraining['title'].apply(preprocesamiento_texto)\n",
    "\n",
    "dataTesting['plot'] = dataTesting['plot'].apply(preprocesamiento_texto)\n",
    "dataTesting['title'] = dataTesting['title'].apply(preprocesamiento_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar embeddings para los titulos y resumenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una función que tokeniza el texto en cada palabra a través de la función .split() y se genera un embedding para cada palabra con fast text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aqui les dejo un resumen otorgado por el amigo del uso de Fast Text y porque lo seleccionamos para hacer los embeddings por encima de otros modelos como Word2Vec o Bert:**\n",
    "\n",
    "//FastText es un modelo de aprendizaje profundo desarrollado por Facebook AI Research (FAIR) que se utiliza principalmente para tareas de procesamiento del lenguaje natural (PLN) como la clasificación de texto y la generación de embeddings de palabras. A continuación, se explica detalladamente su funcionamiento y por qué es especialmente útil en problemas de clasificación:\n",
    "\n",
    "Funcionamiento de FastTe\n",
    "xt\n",
    "Representación de Palabras con N-grama:\n",
    "\n",
    "A diferencia de otros modelos como Word2Vec, FastText descompone las palabras en sub-palabras o n-gramas de caracteres. Por ejemplo, la palabra \"gato\" podría descomponerse en los siguientes trigrama: <ga, gat, ato, to>.\n",
    "Cada palabra se representa como la suma de los vectores de sus n-gramas, lo que permite que el modelo capte información morfológica y semántica a nivel sub-pa\n",
    "labra.\n",
    "Entrenamiento de Modelos:\n",
    "\n",
    "Modelo Skip-Gram: Similar a Word2Vec, FastText utiliza el modelo Skip-Gram, donde la tarea es predecir el contexto de una palabra dada (las palabras que aparecen alrededor de la palabra objetivo en una ventana de co\n",
    "ntexto).\n",
    "Clasificación Jerárquica: Para tareas de clasificación de texto, FastText utiliza una estructura jerárquica de softmax, lo que hace más eficiente la predicción de categorías en un gran conjunto de e\n",
    "tiquetas.\n",
    "Construcción de Embeddings:\n",
    "\n",
    "Los embeddings de palabras se generan a partir de los vectores de los n-gramas que componen las palabras. Esto permite que palabras similares en su forma (morfología) compartan información en sus embeddings.\n",
    "Los embeddings de FastText pueden generalizar mejor para palabras fuera del vocabulario (OOV, por sus siglas en inglés) porque pueden construir representaciones para palabras no vistas previamente basándose en \n",
    "sus n-gramas.\n",
    "Utilidad en Problemas de\n",
    " Clasificación\n",
    "Manejo de Palabrs Raras y OOV:\n",
    "\n",
    "Debido a la descomposición en n-gramas, FastText puede crear representaciones útiles para palabras raras o nuevas que no estaban presentes durante el entrenamiento. Esto es crucial en tareas de clasificación donde el vocabulario puede ser muy va\n",
    "riado y dinámico.\n",
    "Mejora en la Represetación Semántica:\n",
    "\n",
    "La consideración de sub-palabras permite que FastText capture relaciones morfológicas y semánticas entre palabras, lo que mejora la calidad de los embeddings. Esto, a su vez, se traduce en una mejor capacidad de generalización en tareas de cla\n",
    "sificación de texto.\n",
    "Eficencia Computacional:\n",
    "\n",
    "FastText es altamente eficiente en términos de tiempo y recursos computacionales tanto en el entrenamiento como en la inferencia. Utiliza una aproximación jerárquica para reducir el costo computacional en tareas de clasificación\n",
    " con muchas categorías.\n",
    "Adaptabilidad a Diferenes Lenguas y Dialectos:\n",
    "\n",
    "El uso de n-gramas hace que FastText sea especialmente robusto para lenguas con rica morfología o múltiples dialectos, donde las variaciones en\n",
    " las palabras son comunes.\n",
    "Aplicabildad a Tareas Multilingües:\n",
    "\n",
    "Los embeddings de FastText pueden ser entrenados en múltiples lenguas, lo que permite aplicaciones multilingües sin necesidad de modelos separados para cada idioma. Esto es especialmente útil en aplicaciones globales que requieren\n",
    " soporte para varios idio\n",
    "mas.\n",
    "Ejemplos de Aplicacione\n",
    "Clasificación de Documentos:\n",
    "\n",
    "FastText se utiliza para clasificar grandes volúmenes de texto en categorías predefinidas, como la clasificación de noticias, revisión de produ\n",
    "ctos, y análisis desentimientos.\n",
    "Detección de Spam:\n",
    "\n",
    "En sistemas de correo electrónico o redes sociales, FastText puede ayudar a clasificar y filtrar \n",
    "mensajes de spam con alta pecisión.\n",
    "Sistemas de Recomendación:\n",
    "\n",
    "Al comprender las preferencias de los usuarios a través del análisis de texto (por ejemplo, reseñas de productos), FastText puede mejo\n",
    "rar las recomendaciones pFonalizadas.\n",
    "Análisis de Sentimientos:\n",
    "\n",
    "FastText es utilizado para detectar y clasificar opiniones en texto, por ejemplo, para monitorear las reds\n",
    " sociales o analizar reseñas de productos.\n",
    "En resumen, FastText es un modelo poderoso y eficiente para generar embeddings y realizar clasificación de texto, especialmente útil por su capacidad para manejar palabras raras y nuevas a través del uso de// n-gramas y por su eficiencia computacional.e n-gramas y por su eficiencia computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener los embeddings a partir de Fast text\n",
    "\n",
    "def obtener_embedding(texto, modelo):\n",
    "    tokens = texto.split()\n",
    "    embeddings = [modelo.get_word_vector(word) for word in tokens]\n",
    "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(modelo.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de variables\n",
    "\n",
    "X_plot = np.array([obtener_embedding(resumen, ft_model) for resumen in dataTraining['plot']])\n",
    "X_title = np.array([obtener_embedding(titulo, ft_model) for titulo in dataTraining['title']])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataTraining[['year']])\n",
    "\n",
    "X_year = scaler.transform(dataTraining[['year']])\n",
    "\n",
    "X = np.concatenate([X_year, X_title, X_plot], axis=1)\n",
    "\n",
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición y entrenamiento del modelo (Red Neuronal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui se entrena la red Neuronal con los conjuntos X_train y y_train_genres. Queda pendiente calibrar para cargar el modelo, pero este modelo 'Lite' tiene mejor desempeño que el que estaba propuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y) en set de entrenamiento y test usandola función train_test_split\n",
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X, y_genres, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1845 - loss: 0.3880 - val_accuracy: 0.2429 - val_loss: 0.2648\n",
      "Epoch 2/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3236 - loss: 0.2458 - val_accuracy: 0.3242 - val_loss: 0.2290\n",
      "Epoch 3/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3721 - loss: 0.2094 - val_accuracy: 0.3355 - val_loss: 0.2157\n",
      "Epoch 4/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3876 - loss: 0.1947 - val_accuracy: 0.3667 - val_loss: 0.2115\n",
      "Epoch 5/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4182 - loss: 0.1813 - val_accuracy: 0.3450 - val_loss: 0.2086\n",
      "Epoch 6/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4246 - loss: 0.1688 - val_accuracy: 0.3771 - val_loss: 0.2076\n",
      "Epoch 7/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4328 - loss: 0.1595 - val_accuracy: 0.3866 - val_loss: 0.2086\n",
      "Epoch 8/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4490 - loss: 0.1495 - val_accuracy: 0.3582 - val_loss: 0.2078\n",
      "Epoch 9/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4638 - loss: 0.1362 - val_accuracy: 0.3620 - val_loss: 0.2103\n",
      "Epoch 10/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4682 - loss: 0.1296 - val_accuracy: 0.3771 - val_loss: 0.2214\n",
      "Epoch 11/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4810 - loss: 0.1188 - val_accuracy: 0.3677 - val_loss: 0.2223\n",
      "Epoch 12/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4938 - loss: 0.1089 - val_accuracy: 0.3828 - val_loss: 0.2266\n",
      "Epoch 13/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4944 - loss: 0.0991 - val_accuracy: 0.3488 - val_loss: 0.2323\n",
      "Epoch 14/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4974 - loss: 0.0913 - val_accuracy: 0.3422 - val_loss: 0.2415\n",
      "Epoch 15/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5073 - loss: 0.0835 - val_accuracy: 0.3639 - val_loss: 0.2486\n",
      "Epoch 16/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5122 - loss: 0.0740 - val_accuracy: 0.3488 - val_loss: 0.2610\n",
      "Epoch 17/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5319 - loss: 0.0673 - val_accuracy: 0.3374 - val_loss: 0.2715\n",
      "Epoch 18/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5262 - loss: 0.0604 - val_accuracy: 0.3667 - val_loss: 0.2792\n",
      "Epoch 19/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5223 - loss: 0.0523 - val_accuracy: 0.3478 - val_loss: 0.2937\n",
      "Epoch 20/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5234 - loss: 0.0462 - val_accuracy: 0.3544 - val_loss: 0.3171\n",
      "Epoch 21/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5316 - loss: 0.0401 - val_accuracy: 0.3667 - val_loss: 0.3279\n",
      "Epoch 22/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5345 - loss: 0.0359 - val_accuracy: 0.3384 - val_loss: 0.3334\n",
      "Epoch 23/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5460 - loss: 0.0310 - val_accuracy: 0.3573 - val_loss: 0.3469\n",
      "Epoch 24/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5328 - loss: 0.0251 - val_accuracy: 0.3582 - val_loss: 0.3747\n",
      "Epoch 25/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5299 - loss: 0.0221 - val_accuracy: 0.3516 - val_loss: 0.3830\n",
      "Epoch 26/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5300 - loss: 0.0174 - val_accuracy: 0.3507 - val_loss: 0.3972\n",
      "Epoch 27/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5384 - loss: 0.0155 - val_accuracy: 0.3563 - val_loss: 0.4124\n",
      "Epoch 28/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5386 - loss: 0.0133 - val_accuracy: 0.3431 - val_loss: 0.4326\n",
      "Epoch 29/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5322 - loss: 0.0122 - val_accuracy: 0.3440 - val_loss: 0.4440\n",
      "Epoch 30/30\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5305 - loss: 0.0096 - val_accuracy: 0.3412 - val_loss: 0.4652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cbc1b8aae0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Sequential()\n",
    "clf.add(Dense(X.shape[1], input_dim=X.shape[1], activation='relu'))\n",
    "clf.add(Dense(256, activation='relu'))\n",
    "clf.add(Dense(y_genres.shape[1], activation='sigmoid'))  # Sigmoid para multietiqueta\n",
    "\n",
    "clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "clf.fit(X_train, y_train_genres, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8475369125080784"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicción del modelo de clasificación\n",
    "y_pred_genres = clf.predict(X_test)\n",
    "\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test_genres, y_pred_genres, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar archivos Plk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model_deployment/scaler_year.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, './model_deployment/red_neuronal_peliculas.pkl', compress=3)\n",
    "joblib.dump(scaler, './model_deployment/scaler_year.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para predecir probabilidad genero películas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui se define una función que retorna un diccionario con las probabilidades de cada película de pertenecer a cada genero usando los modelos creados y las funciones de obtener embeddings y preprocesamiento de texto construidas anteriormente. En la última celda existe una prueba de funcionamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define la función\n",
    "def probabilidad_genero_pelicula(year, title, plot):\n",
    "\n",
    "    #scaler = StandardScaler()\n",
    "\n",
    "    dicc = {'year': [year],\n",
    "            'title': [title],\n",
    "            'plot': [plot]}\n",
    "    \n",
    "    base = pd.DataFrame(dicc)\n",
    "\n",
    "    base['plot'] = base['plot'].apply(preprocesamiento_texto)\n",
    "    base['title'] = base['title'].apply(preprocesamiento_texto)\n",
    "\n",
    "    X_plot = np.array([obtener_embedding(resumen, ft_model) for resumen in base['plot']])\n",
    "    X_title = np.array([obtener_embedding(titulo, ft_model) for titulo in base['title']])\n",
    "\n",
    "    X_year = scaler.transform(base[['year']])\n",
    "\n",
    "    X = np.concatenate([X_year, X_title, X_plot], axis=1)\n",
    "    \n",
    "    cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "    \n",
    "    #clf = red_neuronal\n",
    "    \n",
    "    proba_genres = clf.predict(X)\n",
    "    df = pd.DataFrame(proba_genres, columns=cols)\n",
    "    diccionario = df.to_dict(orient='list')\n",
    "\n",
    "    return diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p_Action': [1.285593123071381e-16],\n",
       " 'p_Adventure': [1.66012738495383e-13],\n",
       " 'p_Animation': [3.771059908785901e-08],\n",
       " 'p_Biography': [1.5381954654003493e-05],\n",
       " 'p_Comedy': [1.0],\n",
       " 'p_Crime': [3.026908501624348e-08],\n",
       " 'p_Documentary': [0.24686846137046814],\n",
       " 'p_Drama': [8.113729563774541e-07],\n",
       " 'p_Family': [2.6588822810147406e-10],\n",
       " 'p_Fantasy': [1.1559663274339493e-13],\n",
       " 'p_Film-Noir': [1.5108414255909172e-22],\n",
       " 'p_History': [3.4666839304796707e-15],\n",
       " 'p_Horror': [8.648400944346479e-17],\n",
       " 'p_Music': [6.006782768963603e-07],\n",
       " 'p_Musical': [3.0062039968470344e-06],\n",
       " 'p_Mystery': [2.914296581835174e-10],\n",
       " 'p_News': [2.498630208985775e-12],\n",
       " 'p_Romance': [0.007413754239678383],\n",
       " 'p_Sci-Fi': [3.4234014778667553e-19],\n",
       " 'p_Short': [1.159564999397844e-05],\n",
       " 'p_Sport': [1.0731306048319435e-14],\n",
       " 'p_Thriller': [1.3025129722202066e-19],\n",
       " 'p_War': [3.0382564953496116e-12],\n",
       " 'p_Western': [3.379485435510307e-16]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prueba de funcionamiento\n",
    "probabilidad_genero_pelicula('1999', 'Drugs', 'This is a comedy movie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte estoy buscando un método que nos permita calibrar todos los hiper parámetros que contiene el diccionario param_grid. Ya que el KerasClassifier no admite todos los hiperparámetros para iterar a traves de Grid Search o random Search.\n",
    "\n",
    "\n",
    "Tener en cuenta que estas funciones y códigos de esta sección hacia abajo **NO** van en el archivo py para montaje en AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tu función para construir el modelo\n",
    "def build_model(optimizer='adam', init='glorot_normal', dropout_rate=0.2, neurons=350, neurons2=100):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=X.shape[1], kernel_initializer=init, activation='sigmoid'))\n",
    "    model.add(Dense(neurons2, activation='sigmoid'))\n",
    "    model.add(Dropout(dropout_rate, input_shape=(X.shape[1],)))\n",
    "    model.add(Dense(y_genres.shape[1], kernel_initializer=init, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'optimizer': ['rmsprop', 'adam'],\n",
    "    'init': ['glorot_uniform', 'normal', 'he_normal'],\n",
    "    'epochs': [50, 100, 150, 200],\n",
    "    'batch_size': [5, 10, 20, 25],\n",
    "    'dropout_rate': [0.0, 0.1, 0.2, 0.3],\n",
    "    'neurons': [200, 350, 500],\n",
    "    'neurons2':[100, 200, 300]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar predicciones sobre el conjunto test para cargar a la competencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja todo marcado como markdown hasta calibrar el modelo para poder ejecutar todo el codigo de ser necesario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"# transformación variables predictoras X del conjunto de test\"\n",
    "X_test_dtm = vect.transform(dataTesting['plot'])\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "\"# Predicción del conjunto de test\n",
    "y_pred_test_genres = clf.predict_proba(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"# Guardar predicciones en formato exigido en la competencia de kaggle\"\n",
    "res = pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_RF.csv', index_label='ID')\n",
    "res.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
